{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "901d44c1-76ff-427a-a587-2933424f20d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 8175\n",
      "})\n",
      "label=not_spam, sms=hey I am looking for Xray baggage datasets can you provide me with the same \n",
      "label=spam, sms=\"Get rich quick! Make millions in just days with our new and revolutionary system! Don't miss out on this amazing opportunity!\"\n",
      "\n",
      "\n",
      "label=spam, sms=URGENT MESSAGE: YOU WON'T BELIEVE WHAT WE HAVE TO OFFER!!!\n",
      "\n",
      "Hey you! Yeah, you with the eyes reading this right now. Do you want to be the coolest cat on the block? Do you want to get all the likes, hearts and emojis? Do you want to be ~*POPULAR*~? Well, we have the solution for you.\n",
      "\n",
      "Introducing our brand new feature that will blow your mind and your feed! We can't give away too many details, but let's just say it involves puppies, unicorns, and a drum kit.\n",
      "\n",
      "But wait, there's more! Sign up\n"
     ]
    }
   ],
   "source": [
    "# Find a spam dataset at https://huggingface.co/datasets and load it using the datasets library\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# print(load_dataset.__doc__)\n",
    "\n",
    "dataset = load_dataset(\"Deysi/spam-detection-dataset\", split=[\"train\"])[0]\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "for entry in dataset.select(range(3)):\n",
    "    sms = entry[\"text\"]\n",
    "    label = entry[\"label\"]\n",
    "    print(f\"label={label}, sms={sms}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d13439f-5dd8-44b4-adbb-cb340f38e39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_id: spam\n",
      "label=not_spam, sms=hey I am looking for Xray baggage datasets can you provide me with the same \n",
      "label_id: spam\n",
      "label=spam, sms=\"Get rich quick! Make millions in just days with our new and revolutionary system! Don't miss out on this amazing opportunity!\"\n",
      "\n",
      "\n",
      "label_id: spam\n",
      "label=spam, sms=URGENT MESSAGE: YOU WON'T BELIEVE WHAT WE HAVE TO OFFER!!!\n",
      "\n",
      "Hey you! Yeah, you with the eyes reading this right now. Do you want to be the coolest cat on the block? Do you want to get all the likes, hearts and emojis? Do you want to be ~*POPULAR*~? Well, we have the solution for you.\n",
      "\n",
      "Introducing our brand new feature that will blow your mind and your feed! We can't give away too many details, but let's just say it involves puppies, unicorns, and a drum kit.\n",
      "\n",
      "But wait, there's more! Sign up\n"
     ]
    }
   ],
   "source": [
    "# Convenient dictionaries to convert between labels and ids\n",
    "id2label = {0: \"not_spam\", 1: \"spam\"}\n",
    "label2id = {\"not_spam\": 0, \"spam\": 1}\n",
    "\n",
    "for entry in dataset.select(range(3)):\n",
    "    sms = entry[\"text\"]\n",
    "    label = entry[\"label\"]\n",
    "    print(f\"label_id: {label_id}\")\n",
    "    print(f\"label={label}, sms={sms}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c088c33-0ae8-4ad6-aa44-f3818df1d800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (label=spam) -> hey I am looking for Xray baggage datasets can you provide me with the same \n",
      "1 (label=spam) -> \"Get rich quick! Make millions in just days with our new and revolutionary system! Don't miss out on this amazing opportunity!\"\n",
      "\n",
      "\n",
      "2 (label=spam) -> URGENT MESSAGE: YOU WON'T BELIEVE WHAT WE HAVE TO OFFER!!!\n",
      "\n",
      "Hey you! Yeah, you with the eyes reading this right now. Do you want to be the coolest cat on the block? Do you want to get all the likes, hearts and emojis? Do you want to be ~*POPULAR*~? Well, we have the solution for you.\n",
      "\n",
      "Introducing our brand new feature that will blow your mind and your feed! We can't give away too many details, but let's just say it involves puppies, unicorns, and a drum kit.\n",
      "\n",
      "But wait, there's more! Sign up\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's start with this helper function that will help us format sms messages\n",
    "# for the LLM.\n",
    "def get_sms_messages_string(dataset, item_numbers, include_labels=False):\n",
    "    sms_messages_string = \"\"\n",
    "    for item_number, entry in zip(item_numbers, dataset.select(item_numbers)):\n",
    "        sms = entry[\"text\"]\n",
    "        label_id = entry[\"label\"]\n",
    "\n",
    "        if include_labels:\n",
    "            sms_messages_string += (\n",
    "                f\"{item_number} (label={label}) -> {sms}\\n\"\n",
    "            )\n",
    "        else:\n",
    "            sms_messages_string += f\"{item_number} -> {sms}\\n\"\n",
    "\n",
    "    return sms_messages_string\n",
    "\n",
    "\n",
    "print(get_sms_messages_string(dataset, range(3), include_labels=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b47c451-c444-4f29-82c2-dcc21b20dfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please classify the following SMS messages as 'spam' or 'not spam':\n",
      "\n",
      "7 ->  \n",
      "\n",
      "Hi everyone,\n",
      "\n",
      "For my thesis I'm looking for a daily count of subscribers for a subreddit. [https://subredditstats.com/](https://subredditstats.com/) is great but for whatever reasons it excludes a couple of days which I desperately need to have. Does anybody have any way of finding this out?\n",
      "\n",
      "Thank you!!\n",
      "8 -> I'm looking for a large dataset of n-grams (preferably 4-grams or 5-grams, though bigrams or trigrams might also work), in order to get some cooccurrence data. Are there any such datasets (freely) available?\n",
      "\n",
      "Sadly, it looks the Google n-grams datasets (http://googleresearch.blogspot.com/2006/08/all-our-n-gram-are-belong-to-you.html -- not the more recently released book n-grams) now cost $150 + shipping.\n",
      "\n",
      "I thought I'd heard that Bing also provided n-grams datasets (http://research.microsoft.com/en-us/collaboration/focus/cs/web-ngram.aspx), but it looks like all the access is through some limited API, so (unless I'm missing something) I can't find all the cooccurrences for a given word X.\n",
      "\n",
      "Are there any other datasets available / am I misunderstanding what Bing provides? Or is my only option to just pony up and pay for the Google data?\n",
      "9 -> Check out these amazing weight loss pills! They are guaranteed to shed pounds in just days! No need to exercise or eat healthy, just pop a pill and watch the fat melt away! Limited time offer, act now!\n",
      "\n",
      "\n",
      "10 -> SÌµPÌ·AÌµMÌ¸MÌµEÌ·RÌµSÌ¶ ÌµAÌ·TÌµTÌ¶AÌµCÌ¸KÌ¸IÌ¶NÌµGÌ· ÌµOÌ¸NÌ· Ì¶SÌµOÌ¸CÌµIÌ¸AÌµLÌ· ÌµMÌ¶EÌµDÌ·IÌµAÌ¶\n",
      "\n",
      "ThÌ¶eÌ·sÌ·eÌµ Ì¶dÌµaÌ·yÌ¸s\n",
      "11 -> Websites such as Zillow and Redfin display this information, but I'm wondering if someone has compiled a dataset containing this information. \n",
      "\n",
      "I recall years ago that a classmate did a project using this data, being able to include a GIS map.\n",
      "\n",
      " \n",
      "\n",
      "Example of the information:\n",
      "\n",
      "#### Price and tax history\n",
      "\n",
      "##### Price history\n",
      "\n",
      "**Date**   **Event**   **Price**\n",
      "\n",
      "10/25/2007   Sold   $459,000 (+35%)$252/sqftSource: Public Record \n",
      "\n",
      "5/6/2005   Sold   $340,000 (+30.8%)$186/sqftSource: Public Record \n",
      "\n",
      "9/28/2001   Sold   $260,000 (+3614.3%)$142/sqftSource: Public Record \n",
      "\n",
      "12/7/1998   Sold   $7,000 (-96%)$4/sqftSource: Public Record\n",
      "\n",
      "8/2/1996   Sold   $175,000$96/sqftSource: Public Record\n",
      "12 -> HEY EVERYONE!!! ðŸ‘‹ðŸ‘‹ðŸ‘‹\n",
      "\n",
      "Are you Tired ðŸ˜´ of boring ðŸ˜’ posts?? Me too!!! That's why I'm here to spice things up on this app ðŸŒ¶ï¸ðŸŒ¶ï¸ðŸŒ¶ï¸!!!\n",
      "\n",
      "Are you looking for a way to lose weight ðŸ’ªðŸƒâ€â™€ï¸ðŸ¥¦ and gain muscle ðŸ’ªðŸ’ªðŸ’ª? Do not worry, I have the solution for you!!! ðŸ™ŒðŸ™ŒðŸ™Œ\n",
      "\n",
      "Introducing the amazing ðŸ¤©ðŸ¤©ðŸ¤© diet pills that will make you lose weight in\n",
      "13 -> Given the current going-ons was hoping to do some analysis of Use of Force by Police in the US. I found this page on the FBI website: \n",
      "https://www.fbi.gov/services/cjis/ucr/use-of-force\n",
      "\n",
      "Which speaks of a program for use of force data collection nationwide. It both says that a pilot program was performed and the data released, and that the program launched on January 1st, 2019 and they'll release data at least twice per year. \n",
      "\n",
      "However, I can't find any data, either from the pilot or the actual data collection which started more than a year ago. Has anyone come across this dataset? Is it a matter of it being hard to find or has the FBI not actually released the data they said they were going to?\n",
      "14 -> ðŸ‘‹HEY THERE, SOCIAL MEDIA FANATICS!ðŸ’¥\n",
      "It's time to get your ðŸ¤³selfie gameðŸ¤³ on with our amazing platform!ðŸ’»\n",
      "\n",
      "ðŸ”¥Are you tired of boring and low-quality filters on other platforms?ðŸš«\n",
      "Well, we've got you covered with our latest and greatest filters that will make you look ðŸ”¥HOTTER THAN EVERðŸ”¥! \n",
      "\n",
      "ðŸ¤‘And, if you're looking to make some ðŸ’°extra cashðŸ’°, we've got some great news for you! With our referral program, you can earn up to\n",
      "\n",
      "\n",
      "Respond in JSON format with the classification for each message.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace <MASK> with your code\n",
    "\n",
    "# Get a few messages and format them as a string\n",
    "sms_messages_string = get_sms_messages_string(dataset, range(7, 15))\n",
    "\n",
    "# Construct a query to send to the LLM including the sms messages.\n",
    "# Ask it to respond in JSON format.\n",
    "query = f\"\"\"\n",
    "Please classify the following SMS messages as 'spam' or 'not spam':\n",
    "\n",
    "{sms_messages_string}\n",
    "\n",
    "Respond in JSON format with the classification for each message.\n",
    "\"\"\"\n",
    "\n",
    "print(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9ddc2e-934f-40fb-855b-48f6963b1511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace <MASK> with your LLMs response\n",
    "\n",
    "response = <MASK>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
